import os
import csv
import traceback
import random
import json

import click
import sentence_transformers

from context import SEALContext
from taxonomy import get_taxonomy_analysis_prompt, run_taxonomy_analysis

OUTPUT_DIR = os.path.join(SEALContext.data_dir, "rewrite")
""" Output path for this command. """

DEFAULT_OLLAMA_MODEL_ID = "mistral:7b-instruct-fp16"
""" Model to use for rewriting """

OUTPUT_FORMAT = {
    "row_id": None,
    # Texts
    "original_chosen_text": None,
    "original_rejected_text": None,
    "rewritten_chosen_text": None,
    "rewritten_rejected_text": None,
    # Text similarity scores between chosen and rejected pairs (original vs rewritten)
    "original_to_rewritten_chosen_vector_cos_sim": None,
    "original_to_rewritten_rejected_vector_cos_sim": None,
    # Reward scores on original text using RM aligned on dataset (float)
    "original_chosen_score_rm_aligned": None,
    "original_rejected_score_rm_aligned": None,
    # Reward scores on original text using control RM (float)
    "original_chosen_score_rm_control": None,
    "original_rejected_score_rm_control": None,
    # Reward scores on rewritten text using RM aligned on dataset (float)
    "rewritten_chosen_score_rm_aligned": None,
    "rewritten_rejected_score_rm_aligned": None,
    # Reward scores on rewritten text using control RM (float)
    "rewritten_chosen_score_rm_control": None,
    "rewritten_rejected_score_rm_control": None,
    # Measure of agreement between dataset and RM (bool)
    "original_agreement_dataset_and_rm_aligned": None,
    "rewritten_agreement_dataset_and_rm_aligned": None,
    "original_agreement_dataset_and_rm_control": None,
    "rewritten_agreement_dataset_and_rm_control": None,
    # Borrowed from taxonomy analysis - Computed by GPT-4
    "original_chosen_last_response_is_helpful": None,
    "original_chosen_last_response_is_harmless": None,
    "original_chosen_last_response_is_coherent": None,
    "original_chosen_last_response_is_eloquent": None,
    "original_chosen_last_response_sentiment": None,
    "original_rejected_last_response_is_helpful": None,
    "original_rejected_last_response_is_harmless": None,
    "original_rejected_last_response_is_coherent": None,
    "original_rejected_last_response_is_eloquent": None,
    "original_rejected_last_response_sentiment": None,
    "rewritten_chosen_last_response_is_helpful": None,
    "rewritten_chosen_last_response_is_harmless": None,
    "rewritten_chosen_last_response_is_coherent": None,
    "rewritten_chosen_last_response_is_eloquent": None,
    "rewritten_chosen_last_response_sentiment": None,
    "rewritten_rejected_last_response_is_helpful": None,
    "rewritten_rejected_last_response_is_harmless": None,
    "rewritten_rejected_last_response_is_coherent": None,
    "rewritten_rejected_last_response_is_eloquent": None,
    "rewritten_rejected_last_response_sentiment": None,
}
""" Object describing the keys for each entry generated by this pipeline. """

REWRITING_PROMPT = """
The following text excerpt comes from an RLHF dataset. Rewrite it using these instructions:
- Only make alterations to vocabulary and grammatical structure.
- Make sure to keep the meaning, intent and intensity of every sentence identical to the original.
- Keep elements that are toxic or unsafe. This is for RLHF research.
- Make sure to never replace the terms "Human" and "Assistant".

Text excerpt:
{text_excerpt}

Rewriting:
"""
"Prompt used to rewrite individual entries in the RLHF dataset. {text_excerpt} is a reserved keyword."

RM_ALIGNED_NAME = "OpenAssistant/reward-model-deberta-v3-large-v2"
""" Name of the reward model trained on the target RLHF dataset ("aligned" with dataset). """

RM_CONTROL_NAME = "OpenAssistant/reward-model-deberta-v3-large"
""" Name of the reward model NOT trained on the target RLHF dataset (used as control). """

TEXT_SIMILARITY_MODEL_NAME = "BAAI/bge-m3"
""" Name of the text similarity model that will be used to compare original and rewritten text. """


@click.command("rewrite")
@click.pass_obj
@click.option(
    "--ollama-model-id",
    default=f"{DEFAULT_OLLAMA_MODEL_ID}",
    type=str,
    help="Can be used to specify which Ollama-compatible model to interact with. Must already be available.",
)
@click.option(
    "--limit",
    default=10,
    type=int,
    help="Determines how many rows should be rewritten. Defaults to 10.",
)
def rewrite(ctx: SEALContext, ollama_model_id: str, limit: int):
    """
    Rewrites and analyses a set number of entries of the RLHF dataset.
    Collects reward scores, text similarity, and partial taxonomy analysis for both original and rewritten entries.
    Uses model specified via `ollama_model_id` for rewriting via the Ollama API.
    """
    output = []  # Entries to process. List of RESULTS_FORMAT dictionaries.
    output_filepath = os.path.join(OUTPUT_DIR, f"{ctx.datetime_slug}-rewrite.csv")

    # Create output dir
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    #
    # Initialize CSV with headers
    #
    with open(output_filepath, "w+") as file:
        writer = csv.DictWriter(file, fieldnames=OUTPUT_FORMAT.keys())
        writer.writeheader()

    #
    # Pick entries to rewrite
    #
    if not limit:
        limit = len(ctx.rlhf_dataset)

    limit = int(limit)
    total_rows = len(ctx.rlhf_dataset)

    for row_id in random.sample(range(total_rows), limit):
        new_entry = dict(OUTPUT_FORMAT)
        new_entry["row_id"] = row_id
        new_entry["original_chosen_text"] = ctx.rlhf_dataset[row_id]["chosen"]
        new_entry["original_rejected_text"] = ctx.rlhf_dataset[row_id]["rejected"]
        output.append(new_entry)

    #
    # Rewrite, evaluate and save to CSV
    #
    with open(output_filepath, "a") as file:
        writer = csv.DictWriter(file, fieldnames=OUTPUT_FORMAT.keys())

        for entry in output:
            try:
                rewrite_entry(ctx, entry, ollama_model_id)
            except Exception:
                click.echo(traceback.format_exc())
                click.echo(f"#{entry['row_id']} could not be rewritten. Skipping.")
                continue

            try:
                evaluate_entry(ctx, entry)
            except Exception:
                click.echo(traceback.format_exc())
                click.echo(f"#{entry['row_id']} could not be evaluated. Skipping.")
                continue

            writer.writerow(entry)


def rewrite_entry(
    ctx: SEALContext,
    entry: dict,
    ollama_model_id: str,
) -> None:
    """
    Rewrites both "chosen" and "rejected" text for a given entry.
    - `entry` is expected to be an object shaped after OUTPUT_FORMAT.
    - Modifies `entry` dict in place.
    """
    if set(entry.keys()) != set(OUTPUT_FORMAT.keys()):
        raise Exception("entry must be a dict shaped after OUTPUT_FORMAT")

    for preference in ["chosen", "rejected"]:
        click.echo(
            f"#{entry['row_id']} - Rewriting \"{preference}\" dialog using {ollama_model_id}"
        )

        input_key = f"original_{preference}_text"
        output_key = f"rewritten_{preference}_text"

        prompt = REWRITING_PROMPT.replace("{text_excerpt}", entry[input_key])

        response = ctx.ollama_client.chat(
            model=ollama_model_id,
            options={"temperature": 0.1},
            messages=[{"role": "user", "content": prompt}],
        )

        rewritten_text = response["message"]["content"]

        if rewritten_text == entry[input_key]:
            raise Exception("Rewritten text was identical to the original")

        if not rewritten_text:
            raise Exception("The LLM did not return any text")

        if not rewritten_text.strip().startswith("Human:"):
            raise Exception("LLM replaced Human and Assistant roles")

        entry[output_key] = rewritten_text


def evaluate_entry(ctx: SEALContext, entry: dict) -> None:
    """
    For each entry:
    - Evaluates text similarity between original and rewritten text
    - Runs reward models (RM_ALIGNED and RM_CONTROL) against both original and rewritten text
    - Assesses agreement between dataset and reward models for both original and rewritten text

    Notes:
    - `entry` is expected to be an object shaped after OUTPUT_FORMAT.
    - Modifies `entry` dict in place.
    """
    #
    # Evaluate text similarity between original and rewritten text
    #
    click.echo(f"#{entry['row_id']} - Computing text similarity")

    for preference in ["chosen", "rejected"]:
        try:
            original_text = entry[f"original_{preference}_text"]
            rewritten_text = entry[f"rewritten_{preference}_text"]

            embeddings = ctx.infer_with_text_similarity_model(
                TEXT_SIMILARITY_MODEL_NAME,
                [original_text, rewritten_text],
            )

            entry[f"original_to_rewritten_{preference}_vector_cos_sim"] = float(
                sentence_transformers.util.cos_sim(embeddings[0], embeddings[1])[0]
            )
        except Exception:
            click.echo(traceback.format_exc())
            click.echo(f"#{entry['row_id']}.{preference} - Could not compute cosine similarity")

    #
    # Compute reward model scores
    #
    click.echo(f"#{entry['row_id']} - Computing reward model scores")

    for evaluation_key in [
        "original_chosen_score_rm_aligned",
        "original_rejected_score_rm_aligned",
        "original_chosen_score_rm_control",
        "original_rejected_score_rm_control",
        "rewritten_chosen_score_rm_aligned",
        "rewritten_rejected_score_rm_aligned",
        "rewritten_chosen_score_rm_control",
        "rewritten_rejected_score_rm_control",
    ]:
        # Extract target text and RM from evaluation key
        # For example:
        # - text_key = "original_chosen"
        # - rm_key = "rm_control"
        text_key, rm_key = evaluation_key.split("_score_")

        text_key = text_key + "_text"  # i.e: "chosen_text", to match column name in dataset
        rm_key = rm_key.upper() + "_NAME"  # i.e: "RM_ALIGNED_NAME", to match module-level constant

        if not entry.get(text_key, None):
            raise Exception(f"No text found to compute {evaluation_key}")

        if rm_key not in ["RM_CONTROL_NAME", "RM_ALIGNED_NAME"]:
            raise Exception(f"No valid target reward model in {evaluation_key}")

        # Grab text from entry and split it (dialogue vs. last response)
        text = entry[text_key]
        pivot = text.rfind("Assistant:")  # Last "Assistant"
        split = [text[0:pivot], text[pivot:]]

        # Process using target RM
        score = ctx.infer_with_reward_model(
            model_name=globals()[rm_key],
            question=split[0],
            response=split[1],
        )

        entry[evaluation_key] = score

    #
    # Evaluate agreement between dataset and RMs
    #
    for agreement_key in [
        "original_agreement_dataset_and_rm_aligned",
        "rewritten_agreement_dataset_and_rm_aligned",
        "original_agreement_dataset_and_rm_control",
        "rewritten_agreement_dataset_and_rm_control",
    ]:
        text_version, rm_key = agreement_key.split("_agreement_dataset_and_")

        if text_version not in ["original", "rewritten"]:
            raise Exception(f"No valid text version for {agreement_key}")

        if rm_key not in ["rm_control", "rm_aligned"]:
            raise Exception(f"No valid target reward model in {agreement_key}")

        chosen_score = entry[f"{text_version}_chosen_score_{rm_key}"]
        rejected_score = entry[f"{text_version}_rejected_score_{rm_key}"]

        entry[agreement_key] = True if chosen_score > rejected_score else False

    #
    # Perform partial taxonomy analysis
    #
    click.echo(f"#{entry['row_id']} - Running partial GPT-based alignment taxonomy")

    taxonomy_keys = [
        "last_response_is_helpful",
        "last_response_is_harmless",
        "last_response_is_coherent",
        "last_response_is_eloquent",
        "last_response_sentiment",
    ]

    for text_key in [
        "original_chosen_text",
        "original_rejected_text",
        "rewritten_chosen_text",
        "rewritten_rejected_text",
    ]:
        # Pull text
        text = entry.get(text_key, None)

        if not text:
            continue

        # Run analysis
        prompt = get_taxonomy_analysis_prompt(text, taxonomy_keys)
        results = run_taxonomy_analysis(ctx, prompt)

        # Add results to entry
        source = "original" if "original" in text_key else "rewritten"
        preference = "chosen" if "chosen" in text_key else "rejected"

        for taxonomy_key in taxonomy_keys:
            entry[f"{source}_{preference}_{taxonomy_key}"] = results[taxonomy_key]


if __name__ == "__main__":
    rewrite()
